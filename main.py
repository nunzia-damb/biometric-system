# -*- coding: utf-8 -*-
"""Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QkmMFIqhk-M7Tsypi40WpzZnY6756vRC

Dato che da portatile non ho abbastanza potenza di calcolo devo necessariamente usare colab.
Oltretutto devo anche finire di litigare con tensorflow per la GPU.

# Dataset parser
"""


"""# *italicized text*# READMEv

The 136M KEYSTROKES Dataset
===================================
http://userinterfaces.aalto.fi/136Mkeystrokes

This is the 136M Keystrokes dataset.
It contains keystroke data of over 168000 users typing 15 sentences each. The data was collected via an online typing test published at a free typing speed assessment webpage.

More details about the study and its procedure can be found in the paper:

Vivek Dhakal, Anna Maria Feit, Per Ola Kristensson, Antti Oulasvirta
Observations on Typing from 136 Million Keystrokes.
In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI â€™18).

If you have questions, please contact Antti Oulasvirta:
antti.oulasvirta@aalto.fi

----------------------------------
LICENSE AND ATTRIBUTION
----------------------------------

You are free to use this data for non-commercial use in your own research or projects with attribution to the authors.

Please cite:

Vivek Dhakal, Anna Maria Feit, Per Ola Kristensson, Antti Oulasvirta
Observations on Typing from 136 Million Keystrokes.
In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, ACM, 2018.

@inproceedings{dhakal2018observations,
author = {Dhakal, Vivek and Feit, Anna and Kristensson, Per Ola and Oulasvirta, Antti},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18)},
title = {{Observations on Typing from 136 Million Keystrokes}},
year = {2018}
publisher = {ACM}
doi = {https://doi.org/10.1145/3173574.3174220}
keywords = {text entry, modern typing behavior, large-scale study}
}

----------------------------------
CONTENT
----------------------------------
  
- <number>_keystrokes.txt:
	the keystroke-by-keystroke log for all test sentences attempted by the user ID = <number>.

- metadata_participants.txt:
	demographic data of users and aggregate statistics such as speed and errors

----------------------------------
EXPLANATION OF DATA COLUMNS
----------------------------------

- <number>_keystrokes.txt:
PARTICIPANT_ID			Unique ID of participant
TEST_SECTION_ID			Unique ID of the presented sentence
SENTENCE				Sentence shown to the user
USER_INPUT				Sentence typed by the user after pressing Enter or Next button
KEYSTROKE_ID			Unique ID of the keypress
PRESS_TIME				Timestamp of the key down event (in ms)
RELEASE_TIME			Timestamp of the key release event (in ms)
LETTER					The typed letter
KEYCODE					The javascript keycode of the pressed key


- metadata_participants.txt:
The field/column names are described as follows (see paper for details):
PARTICIPANT_ID			Unique ID of participant
AGE
GENDER
HAS_TAKEN_TYPING_COURSE	Whether the participant has taken a typing course (1) or not (0)
COUNTRY
KEYBOARD_LAYOUT			QWERTY, AZERTY or QWERTZ layout of keyboard used
NATIVE_LANGUAGE
FINGERS					Choice between 1-2, 3-4, 5-6, 7-8 and 9-10 fingers used for typing
TIME_SPENT_TYPING		Number of hours spent typing everyday
KEYBOARD_TYPE			Full (desktop), laptop, small physical (e.g on phone) or touch keyboard
ERROR_RATE(%)			Uncorrected error rate
AVG_WPM_15				Words per minute averaged over the 15 typed sentences
AVG_IKI					Average inter-key interval
ECPC					Error Corrections per Character
KSPC					Keystrokes per Character
AVG_KEYPRESS			Average Keypress duration
ROR						Rollover ratio



Note: For some users, Keystrokes are not logged or not displayed correctly. The corresponding javascript keycode is used instead.

# **METADATA**

PARTICIPANT_ID - AGE - GENDER - HAS_TAKEN_TYPING_COURSE - COUNTRY - LAYOUT - NATIVE_LANGUAGE - FINGERS - TIME_SPENT_TYPING - KEYBOARD_TYPE - ERROR_RATE - AVG_WPM_15 - AVG_IKI - ECPC - KSPC - ROR

# **Example of metadata**

```
PARTICIPANT_ID	AGE	GENDER	HAS_TAKEN_TYPING_COURSE	COUNTRY	LAYOUT	NATIVE_LANGUAGE	FINGERS	TIME_SPENT_TYPING	KEYBOARD_TYPE	ERROR_RATE	AVG_WPM_15	AVG_IKI	ECPC	KSPC	ROR
10001	30	none	0	US	qwerty	en	1-2	8	full	0.511945392491468	61.9483	169.224721716287	0.052901023890785	1.1518771331058	0.2288
```

### ***Example of keystroke.txt***

```
PARTICIPANT_ID	TEST_SECTION_ID	SENTENCE	USER_INPUT	KEYSTROKE_ID	PRESS_TIME	RELEASE_TIME	LETTER	KEYCODE
10001	106696	He played goalie almost the entire game.	He played goalie almost the entire game	5088570	1472049642976	1472049643213	SHIFT	16
10001	106696	He played goalie almost the entire game.	He played goalie almost the entire game	5088575	1472049643161	1472049643222	H	72
10001	106696	He played goalie almost the entire game.	He played goalie almost the entire game	5088580	1472049643251	1472049643410	e	69
10001	106696	He played goalie almost the entire game.	He played goalie almost the entire game	5088581	1472049643357	1472049643470	 	32
10001	106696	He played goalie almost the entire game.	He played goalie almost the entire game	5088583	1472049643523	1472049643705	p	80
10001	106696	He played goalie almost the entire game.	He played goalie almost
10001	349	Referring to his meeting in Washington with US President Bill Clinton.	Referring to his meeting in Washington with Us president Bill Clinton.	16800	1471935456171	1471935456264	CAPS_LOCK	20
10001	349	Referring to his meeting in Washington with US President Bill Clinton.	Referring to his meeting in Washington with Us president Bill Clinton.	16801	1471935456751	1471935456874	R	82
10001	349	Referring to his meeting in Washington with US President Bill Clinton.	Referring to his meeting in Washington with Us president Bill Clinton.	16802	1471935457330	1471935457440	CAPS_LOCK	20
10001	349	Referring to his meeting in Washington with US President Bill Clinton.	Referring to his meeting in Washington with Us president Bill Clinton.	16803	1471935458112	1471935458224	e	69
10001	349	Referring to his meeting in Washington with US President Bill Clinton.	Referring to his meeting in Washington with Us president Bill Clinton.	16804	1471935459338	1471935459457	f	70
10001	349	Referring to his meeting in Washington with US President Bill Clinton.	Referring to his meeting in Washington with Us president Bill Clinton.	16805	1471935460960	1471935461060	e	69
10001	349	Referring to his meeting in Washington with US President Bill Clinton.	Referring to his meeting in Washington with Us president Bill Clinton.	16806	1471935461419	1471935461494	r	82
 ```

# **PARSE DATASET**
For now, just to have a softer start, i would suggest to consider only keystrokes without it's original phrase.
Dataset is parsed with this format (for now):<br/>
```
for each user we have a list of lists which contains the users keystrokes for each sentense
[
  id1: [
    [user1_keystroke.txt sentense1 values]
    [user1_keystroke.txt sentense2 values]
    [user1_keystroke.txt sentense3 values]
  ],
  id2: [
    [user2_keystroke.txt sentense1 values]
    [user2_keystroke.txt sentense2 values]
    [user2_keystroke.txt sentense3 values]
  ]
]
```
Where values are [PRESS_TIME - RELEASE_TIME - KEYCODE - ERROR RATE?]

# **What about differnet dimensions?**
For now PADDING
"""

PATH = '/home/tommy/PycharmProjects/biometric-system/data/'
import os
import itertools
from math import floor
keystrokes = ['102_keystrokes.txt', '103_keystrokes.txt', '105_keystrokes.txt']
d = {}

#Parsing dei dati
class UserData(object):
    def __init__(self, lines) -> None:
        # list of phrases
        self.phrases = []
        self.id = int(lines[0].split('\t')[0])
        self._split_phrases(lines)

    #calculate error rate = how many wrong letters (either in substitution or in addition/subtraction) have been typed
    def calc_error_rate(self, user_in, user_out):
        err_rate = 0
        for i in range(min(len(user_in), len(user_out))):
            if user_in[i] != user_out[i]:
                err_rate += 1
        err_rate += abs((len(user_in) - len(user_out)))
        err_rate /= len(user_in)
        return err_rate

    #method that split lines to get datas about sentences
    def _split_phrases(self, lines):
        def select_data(data):
            press_time = data[5]
            release_time = data[6]
            return int(press_time), int(release_time)

        sentence = -1
        data_line = []
        last_sentence1, last_sentence2,  = '', ''
        for l in lines:
            l_split = l.split('\t')
            if int(l_split[1]) != sentence and sentence != -1:
                err_rate = self.calc_error_rate(last_sentence1, last_sentence2)
                # efficiently put the error rate as first value
                # data_line.append(data_line[0])
                # data_line[0] = err_rate
                data_line.append(err_rate)
                self.phrases.append(data_line)
                data_line = []
            last_sentence1, last_sentence2 = l_split[2], l_split[3]

            sentence = int(l_split[1])
            l_split = select_data(l_split)
            data_line.append(l_split)

    #calculate average for press_time, release_time
    def calc_avg(self):
        avg = []
        for i in range(0, len(self.phrases) - 1):
            phrase = self.phrases[i]
            avg_press_time = avg_release_time = 0
            error_rate = self.phrases[i][-1]
            for indx in range(0, len(phrase) - 1):
                press, release = phrase[indx]
                avg_press_time += press
                avg_release_time += release
            avg.append([avg_press_time / len(phrase), avg_release_time / len(phrase), error_rate])
        return avg

    def calc_avg_numpy(self):
        import numpy as np
        means = []
        for phrase in self.phrases:
            a = np.array(phrase[:-1])
            means.append(list(np.mean(a, axis=0)))
        return means

#all lines are parsed and the dict is built
for k in keystrokes:
    with open(PATH + k, 'r', encoding='utf-8') as f:
        from time import time
        start_time = time()
        u = UserData(f.readlines()[1:])
        print(u.phrases)
        avgs = u.calc_avg() #avg_press, avg_release, avg_err
        d[u.id] = avgs
        print('time spent', time() - start_time)
print(d)

#data to use in test and train
test_data = {k:v[floor(len(v)/2):] for k,v in zip(d.keys(),d.values())}

positive_couples = []
for key,value in d.items():
    # combines 2 features vectors for every user
    positive_couples += list(itertools.combinations(d[key][:floor(len(value)/2)], 2))

negative_couples = []
keys = list(d.keys())

#combinations of couples from different subjects
for k1 in range(len(keys)):
    for key2 in list(keys)[k1:]:
      #uses product
      key1 = keys[k1]
      if key2 != key1:
          negative_couples += list(itertools.product(d[key1][:floor(len(value)/2)], d[key2][:floor(len(value)/2)]))


print(positive_couples, len(positive_couples))
print(negative_couples, len(negative_couples))